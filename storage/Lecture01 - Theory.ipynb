{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.anadronestarting.com/wp-content/uploads/intel-main_opt.png' width=50%>\n",
    "\n",
    "# 모바일넷을 이용한 이미지분류\n",
    "<font size=5><b>(Image Classification using Mobilenet)<b></font>\n",
    "\n",
    "<div align='right'>성  민  석<br>(Minsuk Sung)</div>\n",
    "\n",
    "<img src='https://chaosmail.github.io/images/deep-learning/classification.png' width=60%>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>강의목차<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#이미지-분류(Image-Classification)\" data-toc-modified-id=\"이미지-분류(Image-Classification)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>이미지 분류(Image Classification)</a></span></li><li><span><a href=\"#딥러닝-프레임워크(Deep-Learning-Framework)\" data-toc-modified-id=\"딥러닝-프레임워크(Deep-Learning-Framework)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>딥러닝 프레임워크(Deep Learning Framework)</a></span><ul class=\"toc-item\"><li><span><a href=\"#텐서플로우(Tensorflow)\" data-toc-modified-id=\"텐서플로우(Tensorflow)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>텐서플로우(Tensorflow)</a></span></li><li><span><a href=\"#Keras-:-Tensorflow와-손을-잡다\" data-toc-modified-id=\"Keras-:-Tensorflow와-손을-잡다-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Keras : Tensorflow와 손을 잡다</a></span><ul class=\"toc-item\"><li><span><a href=\"#왜-Keras일까요?\" data-toc-modified-id=\"왜-Keras일까요?-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>왜 Keras일까요?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Keras는-사용자-친화적입니다\" data-toc-modified-id=\"Keras는-사용자-친화적입니다-2.2.1.1\"><span class=\"toc-item-num\">2.2.1.1&nbsp;&nbsp;</span>Keras는 사용자 친화적입니다</a></span></li><li><span><a href=\"#Keras는-업계와-학계-양쪽에서-모두-폭넓게-사용되고-있습니다\" data-toc-modified-id=\"Keras는-업계와-학계-양쪽에서-모두-폭넓게-사용되고-있습니다-2.2.1.2\"><span class=\"toc-item-num\">2.2.1.2&nbsp;&nbsp;</span>Keras는 업계와 학계 양쪽에서 모두 폭넓게 사용되고 있습니다</a></span></li><li><span><a href=\"#Keras는-모델의-제품화를-쉽게-해줍니다\" data-toc-modified-id=\"Keras는-모델의-제품화를-쉽게-해줍니다-2.2.1.3\"><span class=\"toc-item-num\">2.2.1.3&nbsp;&nbsp;</span>Keras는 모델의 제품화를 쉽게 해줍니다</a></span></li><li><span><a href=\"#Keras는-여러-백엔드-엔진을-지원하여-하나의-생태계에-속박되지-않습니다\" data-toc-modified-id=\"Keras는-여러-백엔드-엔진을-지원하여-하나의-생태계에-속박되지-않습니다-2.2.1.4\"><span class=\"toc-item-num\">2.2.1.4&nbsp;&nbsp;</span>Keras는 여러 백엔드 엔진을 지원하여 하나의 생태계에 속박되지 않습니다</a></span></li><li><span><a href=\"#Keras는-다중-GPU와-학습의-분산처리를-지원합니다\" data-toc-modified-id=\"Keras는-다중-GPU와-학습의-분산처리를-지원합니다-2.2.1.5\"><span class=\"toc-item-num\">2.2.1.5&nbsp;&nbsp;</span>Keras는 다중 GPU와 학습의 분산처리를 지원합니다</a></span></li><li><span><a href=\"#Keras의-개발은-딥러닝-생태계의-주요-기업들의-지원을-받습니다\" data-toc-modified-id=\"Keras의-개발은-딥러닝-생태계의-주요-기업들의-지원을-받습니다-2.2.1.6\"><span class=\"toc-item-num\">2.2.1.6&nbsp;&nbsp;</span>Keras의 개발은 딥러닝 생태계의 주요 기업들의 지원을 받습니다</a></span></li></ul></li></ul></li><li><span><a href=\"#PyTorch\" data-toc-modified-id=\"PyTorch-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>PyTorch</a></span></li><li><span><a href=\"#Caffe\" data-toc-modified-id=\"Caffe-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Caffe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Caffe란?\" data-toc-modified-id=\"Caffe란?-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Caffe란?</a></span></li><li><span><a href=\"#왜-Caffe일까요?\" data-toc-modified-id=\"왜-Caffe일까요?-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>왜 Caffe일까요?</a></span></li></ul></li></ul></li><li><span><a href=\"#수많은-레이어(Layer)\" data-toc-modified-id=\"수많은-레이어(Layer)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>수많은 레이어(Layer)</a></span><ul class=\"toc-item\"><li><span><a href=\"#완전-연결-레이어(Fully-Connected-Layer)\" data-toc-modified-id=\"완전-연결-레이어(Fully-Connected-Layer)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>완전 연결 레이어(Fully Connected Layer)</a></span></li><li><span><a href=\"#합성곱-레이어(Convolutional-Layer)\" data-toc-modified-id=\"합성곱-레이어(Convolutional-Layer)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>합성곱 레이어(Convolutional Layer)</a></span></li><li><span><a href=\"#최대-풀링-레이어(Max-Pooling-Layer)\" data-toc-modified-id=\"최대-풀링-레이어(Max-Pooling-Layer)-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>최대 풀링 레이어(Max Pooling Layer)</a></span></li><li><span><a href=\"#활성화-함수(Activation-Function)\" data-toc-modified-id=\"활성화-함수(Activation-Function)-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>활성화 함수(Activation Function)</a></span></li></ul></li><li><span><a href=\"#다양한-신경망-네트워크(Neural-Network)\" data-toc-modified-id=\"다양한-신경망-네트워크(Neural-Network)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>다양한 신경망 네트워크(Neural Network)</a></span><ul class=\"toc-item\"><li><span><a href=\"#합성곱-신경망(CNN,-Convolutional-Neural-Network)\" data-toc-modified-id=\"합성곱-신경망(CNN,-Convolutional-Neural-Network)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>합성곱 신경망(CNN, Convolutional Neural Network)</a></span></li></ul></li><li><span><a href=\"#더-깊은-딥러닝-모델(Deep-Learning-Model)\" data-toc-modified-id=\"더-깊은-딥러닝-모델(Deep-Learning-Model)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>더 깊은 딥러닝 모델(Deep Learning Model)</a></span><ul class=\"toc-item\"><li><span><a href=\"#AlexNet\" data-toc-modified-id=\"AlexNet-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>AlexNet</a></span></li><li><span><a href=\"#Inception-V3\" data-toc-modified-id=\"Inception-V3-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Inception V3</a></span></li><li><span><a href=\"#VGG-16\" data-toc-modified-id=\"VGG-16-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>VGG-16</a></span></li><li><span><a href=\"#ResNet-50\" data-toc-modified-id=\"ResNet-50-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>ResNet-50</a></span></li></ul></li><li><span><a href=\"#MobileNet\" data-toc-modified-id=\"MobileNet-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>MobileNet</a></span><ul class=\"toc-item\"><li><span><a href=\"#딥러닝-경량화-기술의-동향\" data-toc-modified-id=\"딥러닝-경량화-기술의-동향-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>딥러닝 경량화 기술의 동향</a></span></li><li><span><a href=\"#MobileNet에-대해서\" data-toc-modified-id=\"MobileNet에-대해서-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>MobileNet에 대해서</a></span></li><li><span><a href=\"#Pipeline\" data-toc-modified-id=\"Pipeline-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Pipeline</a></span></li><li><span><a href=\"#Standard-Convolution-VS-Depthwise-Separable-Convolution\" data-toc-modified-id=\"Standard-Convolution-VS-Depthwise-Separable-Convolution-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Standard Convolution VS Depthwise Separable Convolution</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-Convolution\" data-toc-modified-id=\"Standard-Convolution-6.4.1\"><span class=\"toc-item-num\">6.4.1&nbsp;&nbsp;</span>Standard Convolution</a></span></li><li><span><a href=\"#Depthwise-Separable-Convolution\" data-toc-modified-id=\"Depthwise-Separable-Convolution-6.4.2\"><span class=\"toc-item-num\">6.4.2&nbsp;&nbsp;</span>Depthwise Separable Convolution</a></span><ul class=\"toc-item\"><li><span><a href=\"#Depthwise-Convolution\" data-toc-modified-id=\"Depthwise-Convolution-6.4.2.1\"><span class=\"toc-item-num\">6.4.2.1&nbsp;&nbsp;</span>Depthwise Convolution</a></span></li><li><span><a href=\"#Pointwise-Convolution\" data-toc-modified-id=\"Pointwise-Convolution-6.4.2.2\"><span class=\"toc-item-num\">6.4.2.2&nbsp;&nbsp;</span>Pointwise Convolution</a></span></li></ul></li></ul></li><li><span><a href=\"#효율성\" data-toc-modified-id=\"효율성-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>효율성</a></span></li></ul></li><li><span><a href=\"#MobileNetV2\" data-toc-modified-id=\"MobileNetV2-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>MobileNetV2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pipeline\" data-toc-modified-id=\"Pipeline-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Pipeline</a></span></li><li><span><a href=\"#Linear-Bottlenecks\" data-toc-modified-id=\"Linear-Bottlenecks-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Linear Bottlenecks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Two-assumptions-(non-linear-function-을-똑같이-사용-할-수-있다-)\" data-toc-modified-id=\"Two-assumptions-(non-linear-function-을-똑같이-사용-할-수-있다-)-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>Two assumptions (non-linear function 을 똑같이 사용 할 수 있다 )</a></span></li><li><span><a href=\"#In-experiments\" data-toc-modified-id=\"In-experiments-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>In experiments</a></span></li></ul></li><li><span><a href=\"#Inverted-Residuals\" data-toc-modified-id=\"Inverted-Residuals-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Inverted Residuals</a></span><ul class=\"toc-item\"><li><span><a href=\"#Residual-block-VS-Inverted-residual-block\" data-toc-modified-id=\"Residual-block-VS-Inverted-residual-block-7.3.1\"><span class=\"toc-item-num\">7.3.1&nbsp;&nbsp;</span>Residual block VS Inverted residual block</a></span></li><li><span><a href=\"#확장(Expansion)이-필요한-이유는?\" data-toc-modified-id=\"확장(Expansion)이-필요한-이유는?-7.3.2\"><span class=\"toc-item-num\">7.3.2&nbsp;&nbsp;</span>확장(Expansion)이 필요한 이유는?</a></span></li><li><span><a href=\"#Structure-of-inverted-residual-block\" data-toc-modified-id=\"Structure-of-inverted-residual-block-7.3.3\"><span class=\"toc-item-num\">7.3.3&nbsp;&nbsp;</span>Structure of inverted residual block</a></span></li></ul></li><li><span><a href=\"#Memory-Efficient-Inference\" data-toc-modified-id=\"Memory-Efficient-Inference-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Memory Efficient Inference</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bottleneck-Residual-Block\" data-toc-modified-id=\"Bottleneck-Residual-Block-7.4.1\"><span class=\"toc-item-num\">7.4.1&nbsp;&nbsp;</span>Bottleneck Residual Block</a></span></li><li><span><a href=\"#Compute-graph-G\" data-toc-modified-id=\"Compute-graph-G-7.4.2\"><span class=\"toc-item-num\">7.4.2&nbsp;&nbsp;</span>Compute graph G</a></span></li><li><span><a href=\"#t-way-split\" data-toc-modified-id=\"t-way-split-7.4.3\"><span class=\"toc-item-num\">7.4.3&nbsp;&nbsp;</span>t-way split</a></span></li><li><span><a href=\"#성능\" data-toc-modified-id=\"성능-7.4.4\"><span class=\"toc-item-num\">7.4.4&nbsp;&nbsp;</span>성능</a></span></li></ul></li></ul></li><li><span><a href=\"#여러-CNN-모델-파라미터-및-성능-비교\" data-toc-modified-id=\"여러-CNN-모델-파라미터-및-성능-비교-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>여러 CNN 모델 파라미터 및 성능 비교</a></span></li><li><span><a href=\"#Keras에서-제공하는-모델들\" data-toc-modified-id=\"Keras에서-제공하는-모델들-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Keras에서 제공하는 모델들</a></span><ul class=\"toc-item\"><li><span><a href=\"#ImageNet으로-학습한-가중치를-이용해-이미지-분류를-수행하는-모델:\" data-toc-modified-id=\"ImageNet으로-학습한-가중치를-이용해-이미지-분류를-수행하는-모델:-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>ImageNet으로 학습한 가중치를 이용해 이미지 분류를 수행하는 모델:</a></span></li><li><span><a href=\"#예시\" data-toc-modified-id=\"예시-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>예시</a></span></li></ul></li><li><span><a href=\"#Appendix\" data-toc-modified-id=\"Appendix-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Appendix</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-Convolution\" data-toc-modified-id=\"Standard-Convolution-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Standard Convolution</a></span><ul class=\"toc-item\"><li><span><a href=\"#디지털-이미지\" data-toc-modified-id=\"디지털-이미지-10.1.1\"><span class=\"toc-item-num\">10.1.1&nbsp;&nbsp;</span>디지털 이미지</a></span></li><li><span><a href=\"#Convolution-Neural-Network\" data-toc-modified-id=\"Convolution-Neural-Network-10.1.2\"><span class=\"toc-item-num\">10.1.2&nbsp;&nbsp;</span>Convolution Neural Network</a></span></li></ul></li><li><span><a href=\"#Process-of-Convolution\" data-toc-modified-id=\"Process-of-Convolution-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Process of Convolution</a></span></li><li><span><a href=\"#ReLu6\" data-toc-modified-id=\"ReLu6-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>ReLu6</a></span></li></ul></li><li><span><a href=\"#참고\" data-toc-modified-id=\"참고-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>참고</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 이미지 분류(Image Classification)\n",
    "\n",
    "이미지 분류(Image Classification)는 말 그대로 수많은 이미지들을 분류하는 작업입니다. 예를 들면, 수많은 개나 고양이와 같은 이미지를 학습시켜서 처음 보는 사진 속 동물이 개 혹은 고양이인지를 분류할 수 있습니다. 물론, 과거에도 이미지를 분류하려는 수많은 시도가 있었지만, 이미지 분류 문제는 딥러닝(Deep Learning)의 등장을 전후로 크게 발전했습니다.\n",
    "\n",
    "<img src='https://storage.googleapis.com/kaggle-organizations/768/thumbnail.png?r=587'>\n",
    "\n",
    "2012년 캐나다 토론토대학의 알렉스 크리제브스키(Alex Khrizevsky)가 이미지넷(ImageNet)이라 불리는 이미지 인식 경진 대회에서 GPU를 활용한 앞서 언급했던 딥러닝을 이용하여 정확도를 획기적으로 높인 논문을 발표하면서 또 한번의 전환점을 맞이합니다. 이 때 등장한 네트워크가 바로 알렉스넷(AlexNet)입니다. 알렉스넷은 추후 다시 간략하게 설명하겠습니다.\n",
    "\n",
    "![](http://www.pyimagesearch.com/wp-content/uploads/2016/05/deep_learning_example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 딥러닝 프레임워크(Deep Learning Framework)\n",
    "\n",
    "<br>\n",
    "<img src='https://cdn.pixabay.com/photo/2017/09/08/19/07/a-2729794_960_720.png' width=50%>\n",
    "\n",
    " 이미지 분류를 하기 위해서 딥러닝을 이용하면 훨씬 좋은 퍼포먼스를 발휘할 수 있습니다. 그럼 여기서 우리는 의문점이 하나 들게 됩니다.\n",
    " > 그럼 어떻게 딥러닝을 시작하면 좋을까요?\n",
    " \n",
    " 여러분이 만약 수학적인 기초가 튼튼하게 월등한 코딩 실력이 갖춰져있다면, 직접 밑바닥부터 하나하나 직접 구현하셔도 괜찮습니다. 하지만 이렇게 번거로운 방법은 시간이 너무 낭비가 되버립니다. 이미 모든 내용을 다 알고 있는 딥러닝 실력자들일지라도 그러한 것들을 다시 구현한다면, 그들이 원래 하고자하는 일을 할 시간이 줄어들기 때문이죠.  예를 들어, 여러분은 호텔의 수석 요리사라고 합시다. 남부럽지 않는 요리 실력과 고급 요리에 대한 레시피도 다 알고 있다고 가정합시다. 이러한 상황에서 굳이 주방기구를 만들 필요가 있을까요? 칼이나 후라이팬과 같은 요리기구까지 직접 만들어서 요리를 해야하나요? 물론 절대 그럴 필요없죠.\n",
    " \n",
    "> 마찬가지로 `딥러닝 프레임워크(Deep Learning Framework)`는 작업자가 딥러닝을 하기 위한 도구를 모두 모아놓은 도구세트라고 생각하시면 됩니다.\n",
    " \n",
    " 그래서 이미 수많은 회사에서 혹은 연구실에서 딥러닝 프레임워크를 개발하고 있습니다. 우리는 그들의 도움을 받아서 딥러닝을 이용만 하면 됩니다. 앞으로 소개하는 몇가지 프레임워크는 가장 많이 쓰이는 프레임워크들입니다. \n",
    " \n",
    " ![](https://www.nvidia.com/content/dam/en-zz/es_em/Solutions/deep-learning/deep-learning-developer/deep-learning-developer-frameworks-407.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로우(Tensorflow)\n",
    "\n",
    "<img src='https://www.tensorflow.org/images/tf_logo_social.png' width=80%>\n",
    "\n",
    "> 텐서플로우(TensorFlow&#8482;)는 데이터 플로우 그래프(Data flow graph)를 사용하여 수치 연산을 하는 오픈소스 소프트웨어 라이브러리입니다. 그래프의 노드(Node)는 수치 연산을 나타내고 엣지(edge)는 노드 사이를 이동하는 다차원 데이터 배열(텐서,tensor)를 나타냅니다. 유연한 아키텍처로 구성되어 있어 코드 수정없이 데스크탑, 서버 혹은 모바일 디바이스에서 CPU나 GPU를 사용하여 연산을 구동시킬 수 있습니다. 텐서플로우는 원래 머신러닝과 딥 뉴럴 네트워크 연구를 목적으로 구글의 인공지능 연구 조직인 구글 브레인 팀의 연구자와 엔지니어들에 의해 개발되었습니다. 하지만 이 시스템은 여러 다른 분야에도 충분히 적용될 수 있습니다.\n",
    "\n",
    "출처 : 텐서플로우 공식홈페이지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Keras : Tensorflow와 손을 잡다\n",
    "\n",
    "![](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n",
    "\n",
    ">케라스는  \n",
    "많은 이들이 딥러닝을 쉽게 접할 수 있도록,  \n",
    "다양한 플랫폼 위에서 딥러닝 모델을 만들 수 있는   \n",
    "API이다.\n",
    "\n",
    "#### 왜 Keras일까요?\n",
    "\n",
    "오늘날 존재하는 수많은 딥러닝 프레임워크들 중에서, 왜 굳이 Keras일까요? 다른 대안들에 비해 Keras를 선호하는 이유는 다음과 같습니다.\n",
    "\n",
    "---\n",
    "\n",
    "##### Keras는 사용자 친화적입니다\n",
    "    \n",
    "- Keras는 기계가 아닌 사람을 위한 도구입니다. Keras는 [사용자의 부담을 덜기 위해](https://blog.keras.io/user-experience-design-for-apis.html) 일관되고 간결한 API를 제공하며, 일반적인 유스케이스에 필요한 사용자의 조작을 최소화 하고, 오작동에 대한 명확하고 실용적인 피드백을 제공합니다.\n",
    "\n",
    "- Keras의 이런 개발 철학 덕분에 Keras는 배우기도, 사용하기에도 쉽습니다. Keras를 통해서 더 많은 아이디어를 빠르게 시도해 볼 수 있고, 이는 [머신러닝 대회에서 좋은 성적을 거둘 수 있도록 도와줍니다](https://www.quora.com/Why-has-Keras-been-so-successful-lately-at-Kaggle-competitions).\n",
    "\n",
    "- Keras는 쉬운 고수준의 API를 제공하면서도, TensorFlow와 같은 저수준의 API와도 호환이 잘 되어 어떠한 네트워크 구조도 만들 수 있게 합니다. 특히, `tf.keras`를 사용하면 TensorFlow 기반의 작업 흐름에도 매끄럽게 통합시킬 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "##### Keras는 업계와 학계 양쪽에서 모두 폭넓게 사용되고 있습니다\n",
    "\n",
    "<a href='https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a'>\n",
    "    <img style='width: 80%; margin-left: 10%;' src='https://s3.amazonaws.com/keras.io/img/dl_frameworks_power_scores.png'/>\n",
    "</a>\n",
    "<p style='font-style: italic; font-size: 10pt; text-align: center;'>\n",
    "    7개의 분류에 걸친 11개의 데이터 소스를 기반으로 계산된 딥러닝 프레임워크 순위, Jeff Hale.\n",
    "</i>\n",
    "\n",
    "Keras는 250,000명 이상의 개인 사용자(2018년 기준)를 기반으로 TensorFlow를 제외한 그 어떤 딥러닝 프레임워크보다 업계와 학계 모두에 깊게 배어있습니다. 또한 Keras API는 `tf.keras` 모듈을 통해 TensorFlow의 공식 프론트엔드로 사용되고 있습니다.\n",
    "\n",
    "Keras를 통해 개발된 기능들은 Netflix, Uber, Yelp, Instacart, Zocdoc, Square사 등의 서비스에서 쉽게 찾아볼 수 있습니다. 이는 특히 딥러닝을 서비스의 핵심으로 삼는 스타트업 기업들 사이에서 인기가 많습니다.\n",
    "\n",
    "Keras는 [arXiv.org](https://arxiv.org/archive/cs)에 업로드 된 과학 논문들 중에서 두 번째로 많이 언급 될 정도로 딥러닝 연구자들에게 사랑받고 있습니다. Keras는 또한 CERN과 NASA와 같은 대형 연구소에서도 채택된 도구입니다.\n",
    "\n",
    "---\n",
    "\n",
    "##### Keras는 모델의 제품화를 쉽게 해줍니다\n",
    "\n",
    "Keras는 다른 어떤 딥러닝 프레임워크보다도 다양한 방면의 플랫폼에 쉽게 배포할 수 있습니다. 이에 해당하는 플랫폼들은 다음과 같습니다.\n",
    "\n",
    "- iOS에서는 [Apple’s CoreML](https://developer.apple.com/documentation/coreml)을 통해서 가능합니다. Apple사는 공식적으로 Keras를 지원합니다 ([튜토리얼](https://www.pyimagesearch.com/2018/04/23/running-keras-models-on-ios-with-coreml/)). \n",
    "- Android에서는 TensorFlow Android 런타임을 통해서 가능합니다 (e.g. [Not Hotdog 앱](https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3)).\n",
    "- 웹 브라우저에서는 [Keras.js](https://transcranial.github.io/keras-js/#/)와 같은 GPU 가속된 JavaScript 런타임과 [WebDNN](https://mil-tokyo.github.io/webdnn/)을 통해서 가능합니다.\n",
    "- Google Cloud에서는 [TensorFlow-Serving](https://www.tensorflow.org/serving/)을 통해서 가능합니다.\n",
    "- [Flask 앱과 같은 Python 웹 백엔드](https://blog.keras.io/building-a-simple-keras-deep-learning-rest-api.html)에서도 가능합니다.\n",
    "- JVM에서는 [SkyMind가 제공하는 DL4J](https://deeplearning4j.org/model-import-keras)를 통해서 가능합니다.\n",
    "- Raspberry Pi에서도 가능합니다.\n",
    "\n",
    "---\n",
    "\n",
    "##### Keras는 여러 백엔드 엔진을 지원하여 하나의 생태계에 속박되지 않습니다\n",
    "\n",
    "Keras 모델은 여러 [딥러닝 백엔드](https://keras.io/backend/)를 지원합니다. 눈여겨볼 만한 점은, 내장 레이어로만 구성된 Keras 모델들은 지원하는 모든 백엔드들과 호환이 되어 학습에 사용되는 백엔드와 배포 등을 위한 로드에 사용되는 백엔드가 서로 달라도 된다는 것입니다. 사용 가능한 백엔드들은 다음과 같습니다.\n",
    "\n",
    "- TensorFlow 백엔드 (Google사 제공)\n",
    "- CNTK 백엔드 (Microsoft사 제공)\n",
    "- Theano 백엔드\n",
    "\n",
    "Amazon사는 MXNet을 백엔드로 사용하는 [Keras의 분기 버전](https://github.com/awslabs/keras-apache-mxnet)을 제공합니다.\n",
    "\n",
    "결과적으로 Keras 모델들은 CPU뿐만이 아닌 다른 여러 하드웨어 플랫폼들에서도 학습이 가능합니다.\n",
    "\n",
    "- [NVIDIA GPUs](https://developer.nvidia.com/deep-learning)\n",
    "- [Google TPUs](https://cloud.google.com/tpu/) (TensorFlow 백엔드와 Google Cloud를 통해서)\n",
    "- AMD사의 OpenCL과 호환되는 GPU ([PlaidML Keras 백엔드](https://github.com/plaidml/plaidml)를 통해서)\n",
    "\n",
    "---\n",
    "\n",
    "##### Keras는 다중 GPU와 학습의 분산처리를 지원합니다\n",
    "\n",
    "- Keras는 [다중 GPU 데이터 병렬성에 대한 지원이 내장되어있습니다](/utils/#multi_gpu_model).\n",
    "- Uber사의 [Horovod](https://github.com/uber/horovod)는 케라스 모델을 일차적으로 지원합니다.\n",
    "- Keras 모델을 [TensorFlow 추정자로 변환](https://www.tensorflow.org/versions/master/api_docs/python/tf/keras/estimator/model_to_estimator)이 가능하며, [Google Cloud를 통한 GPU 클러스터](https://cloud.google.com/solutions/running-distributed-tensorflow-on-compute-engine)에서 학습시킬 수 있습니다.\n",
    "- [Dist-Keras](https://github.com/cerndb/dist-keras)와 [Elephas](https://github.com/maxpumperla/elephas)를 통해 Spark에서 Keras를 실행할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "##### Keras의 개발은 딥러닝 생태계의 주요 기업들의 지원을 받습니다\n",
    "\n",
    "Keras는 Google사의 지원을 중심으로 개발되고 있으며, Keras API는 `tf.keras`로 TensorFlow의 패키지로 제공됩니다. CNTK Keras 백엔드의 유지보수 또한 Microsoft사의 책임하에 이루어집니다. Amazon AWS는 MXNet과 함께 Keras를 관리합니다. NVIDIA, Uber, CoreML을 포함한 Apple사 또한 Keras의 개발에 공헌하였습니다.\n",
    "\n",
    "<img src='https://keras.io/img/google-logo.png' style='width:200px; margin-right:15px;'/>\n",
    "<img src='https://keras.io/img/microsoft-logo.png' style='width:200px; margin-right:15px;'/>\n",
    "<img src='https://keras.io/img/nvidia-logo.png' style='width:200px; margin-right:15px;'/>\n",
    "<img src='https://keras.io/img/aws-logo.png' style='width:110px; margin-right:15px;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "\n",
    "![](https://pacientes.github.io/2019/07/15/Programming-Python-PyTorch-2019-07-15-python-pytorch-tutorials-03/logo.png)\n",
    "\n",
    "PyTorch는 Python을 위한 오픈소스 머신 러닝 라이브러리이다. Torch를 기반으로 하며[1][2][3], 자연어 처리와 같은 애플리케이션을 위해 사용된다.[4] GPU사용이 가능하기 때문에 속도가 상당히 빠르다. 아직까지는 Tensorflow의 사용자가 많지만, 비직관적인 구조와 난이도 때문에, Pytorch의 사용자가 늘어나고 있는 추세이다. 이는 Facebook의 인공지능 연구팀이 개발했으며, Uber의 “Pyro”(확률론적 프로그래밍 언어)소프트웨어가 Pytorch를 기반으로 한다\n",
    "\n",
    "Pytorch는 두 개의 높은 수준의 파이선 패키지 형태로 제공한다.[5]\n",
    "- 강력한 GPU가속화를 통한 Tensor계산 ex) NumPy\n",
    "- 테이프 기반 자동 삭제 시스템을 기반으로 구축된 심층 신경망\n",
    "\n",
    "Facebook은 PyTorch와 Convolutional Architecture for Fast Feature Embedding (Caffe2)을 모두 운영하고 있지만 비호환성으로 인해 PyTorch 정의 모델을 Caffe2로 변환하거나 그 반대로 변환하는 것이 어렵다. 개신경망 교환(ONNX, Open Neural Network Exchange) 프로젝트는 Facebook과 Microsoft가 프레임워크 간 모델 전환을 위해 2017년 9월 만든 프로젝트다. Caffe2는 2018년 3월 말에 PyTorch으로 합병되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caffe\n",
    "\n",
    "<img src='https://miro.medium.com/max/1600/1*TKh1O_vDYwfsW3JeEYFYlg.jpeg' width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caffe란?\n",
    "Caffe는 표현, 속도 및 모듈성을 염두에두고 만들어진 딥 러닝 프레임 워크입니다. BAIR (Berkeley AI Research )와 커뮤니티 기고자들이 개발했습니다. Yangqing Jia 는 UC Berkeley에서 박사 과정 중에 프로젝트를 만들었습니다. Caffe는 BSD 2-Clause 라이센스에 따라 배포 됩니다.\n",
    "\n",
    "#### 왜 Caffe일까요?\n",
    "`표현형 아키텍처(Expressive architecture)`는 애플리케이션과 혁신을 장려합니다. 모델 및 최적화는 하드 코딩없이 구성으로 정의됩니다. GPU 시스템에서 학습하도록 단일 플래그를 설정하여 CPU와 GPU 간을 전환 한 다음 상용 클러스터 또는 모바일 장치에 배포하십시오.\n",
    "\n",
    "`확장 가능한 코드(Extensible code)`는 적극적인 개발을 촉진합니다. Caffe의 첫 해에는 1,000 명 이상의 개발자가 포크했으며 많은 중요한 변화가있었습니다. 이러한 기여자 덕분에 프레임 워크는 코드와 모델 모두에서 최신 기술을 추적합니다.\n",
    "\n",
    "Caffe는 `속도(Speed)`를 통해 연구 실험 및 산업 배치에 적합합니다. Caffe는 단일 NVIDIA K40 GPU 로 매일 60M 이상의 이미지를 처리 할 수 있습니다 . 추론의 경우 1ms / 이미지이고 학습의 경우 4ms / 이미지이며 최신 라이브러리 버전 및 하드웨어는 여전히 더 빠릅니다. 우리는 Caffe가 가장 빠른 convnet 구현 중 하나라고 생각합니다.\n",
    "\n",
    "`커뮤니티(Community)` : Caffe는 이미 비전, 연설 및 멀티미디어 분야의 학술 연구 프로젝트, 스타트 업 프로토 타입 및 대규모 산업 응용 프로그램을 지원합니다. caffe-users 그룹 및 Github 의 브루어 커뮤니티에 참여하십시오 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 수많은 레이어(Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 완전 연결 레이어(Fully Connected Layer)\n",
    "\n",
    "딥러닝과 신경망 구조를 공부하다보면 가장 먼저 접하게 되는 레이어입니다. 이전 레이어의 모든 노드가 다음 레이어의 모든 노드에 연결된 레이어를 완전 연결 레이어(Fully Connected Layer)라고 합니다. Keras에서는 특별히 이러한 층을 Dense라고도 부릅니다.\n",
    "\n",
    "<img src='https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0401.png' width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 레이어(Convolutional Layer)\n",
    "\n",
    "합성곱 레이어는 이미지에서 필터(Filter,Kernel)를 통해서 주요한 특징을 추출하는 레이어라고 생각하면 된다. 각 레이어마다 수많은 필터를 가지고 있으며, 해당 필터만 추출되는 특징은 그만큼 다양하다. 어떤 필터는 수직 엣지를 추출하는 필터도 있고, 어떤 필터는 수평 엣지를 추출하는 필터도 있다. 이러한 합성곱 레이어가 여러겹으로 쌓일수록 추출할 수 있는 특징 또한 고차원화된다. 쉽게 말하면 합성곱 레이어를 적게 통과한 이미지에서 뽑힌 특징은 저차원 특징을 추출하지만, 합성곱 레이어를 많이 통과한 이미지는 더욱 고차원의 특징을 추출한다.\n",
    "\n",
    "![](https://miro.medium.com/max/790/1*1okwhewf5KCtIPaFib4XaA.gif)\n",
    "\n",
    "![](https://sailinglab.github.io/pgm-spring-2019/assets/img/notes/lecture-16/cnn_hierarchy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최대 풀링 레이어(Max Pooling Layer)\n",
    "\n",
    "위에 합성곱 레이어를 통과할 때 각 영역별로 수많은 값을 가진 특징맵이 생겨나는데, 그 중에서도 가장 두드러지는 특징을 고르는게 바로 최대 풀링 레이어의 역할이다. 최대 풀링 레이어를 거치면 또한 고려해야하는 특징의 수를 줄여주는 역할도 하므로써 연산량을 줄여주는 효과도 생긴다.\n",
    "\n",
    "![](https://www.nvestlabs.com/wp-content/uploads/2019/05/Maxpooling2d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활성화 함수(Activation Function)\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 다양한 신경망 네트워크(Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 신경망(CNN, Convolutional Neural Network)\n",
    "\n",
    "![](https://miro.medium.com/max/602/0*oG-kTxgZ0AHMQ5T1)\n",
    "\n",
    " 앞서 언급한 3개의 레이어를 통해서, 우리는 드디어 이미지를 분류할 수 있는 모델을 만들 준비가 되었습니다. 레이어가 레고 블럭이라면 네트워크는 레고 블럭으로 이루어진 집 정도가 된다고 생각하시면 좋습니다. 합성곱 신경망은 일단 합성곱 레이어를 통해서 이미지의 특징을 뽑아냅니다. 그리고 최대 풀링 레이어를 통해서 가장 두드러진 특징을 찾아냅니다. 합성곱 레이어가 여러겹으로 쌓일수록 이미지에서 뽑을 수 있는 특징은 점점 저차원에서 고차원의 특징까지 뽑을 수 있게 됩니다. 이렇게 합성곱 레이어와 최대 풀링 레이어의 반복을 여러번 반복한 후에는 마지막으로 완전 연결 계층을 통해서 해당 이미지의 클래스를 분류하는 작업을 합니다.\n",
    "\n",
    "![](http://tykimos.github.io/warehouse/2017-9-30-Book_Python_DeepLearning_Keras_with_Block_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 더 깊은 딥러닝 모델(Deep Learning Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/99FEB93C5C80B5192E)\n",
    "\n",
    "Recurrent Neural Network와 더불어 딥러닝 모델의 양대 산맥으로 주목받고 있는 CNN은 기본적으로 얀 르쿤이 1989년 제안한 구조를 토대로 하고 있습니다. 컴퓨터 비전 분야의 ‘올림픽’이라 할 수 있는 ILSVRC(ImageNet Large-Scale Visual Recognition Challenge)의 2012년 대회에서 제프리 힌튼 교수팀의 AlexNet이 top 5 test error 기준 15.4%를 기록해 2위(26.2%)를 큰 폭으로 따돌리고 1위를 차지했습니다.\n",
    "\n",
    "여기서 top 5 test error란 모델이 예측한 최상위 5개 범주 가운데 정답이 없는 경우의 오류율을 나타냅니다. 당시 ILSVRC 데이터셋(Image은 1000개 범주 예측 문제였습니다. 어쨌든 AlexNet 덕분에 딥러닝, 특히 CNN이 세간의 주목을 받게 됐습니다. AlexNet 아키텍처의 주요 특징은 다음과 같습니다.\n",
    "\n",
    "AlexNet이 중요한 이유는 의미있는 성능을 낸 첫번째 CNN 아키텍처이자, AlexNet에 쓰인 드롭아웃 등 기법은 이 분야 표준으로 자리잡을 정도로 선도적인 역할을 했기 때문입니다.\n",
    "\n",
    "출처 : [ratsgo's blog](https://ratsgo.github.io/deep%20learning/2017/10/09/CNNs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception V3\n",
    "\n",
    "![](https://miro.medium.com/max/2800/0*rbWRzjKvoGt9W3Mf.png)\n",
    "\n",
    "AlexNet 이후 층을 더 깊게 쌓아 성능을 높이려는 시도들이 계속되었습니다. VGGNet(2014), GoogleNet(2015) 등이 바로 그것입니다. GoogleNet은 VGGNet보다 구조가 복잡해 널리 쓰이진 않았지만 아키텍처 면에서 주목을 받았습니다. 보통 하나의 conv layer에는 한 가지의 conv filter가 사용됩니다.\n",
    "\n",
    "GoogleNet 연구진들은 한 가지의 conv filter를 적용한 conv layer를 단순히 깊게 쌓는 방법도 있지만, 하나의 layer에서도 다양한 종류의 filter나 pooling을 도입함으로써 개별 layer를 두텁게 확장시킬 수 있다는 창조적인 아이디어로 후배 연구자들에게 많은 영감을 주었습니다. 이들이 제안한 구조가 바로 Inception module입니다. (그림 출처)\n",
    "\n",
    "<img src='https://i.imgur.com/VY3BkBR.png' width=50%>\n",
    "\n",
    "Inception module에서 특히 주목받은 것이 바로 1×1 conv filter입니다. 가령 현재 층 입력데이터 이미지의 차원수가 100×100×60이고, 1×1 conv filter를 20개 사용한다면 데이터의 차원 수는 100×100×20으로 줄어듭니다. 60개 채널(차원)에 달하는 하나의 픽셀이 20차원의 feature space로 선형변환, 차원축소된 것이라고도 볼 수 있겠습니다.\n",
    "\n",
    "출처 : [ratsgo's blog](https://ratsgo.github.io/deep%20learning/2017/10/09/CNNs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16\n",
    "![](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)\n",
    "\n",
    "VGGNet의 특징은 작은 필터 크기의 convolution 연산이다. AlexNet의 경우, 첫번째 convolution layer의 필터 크기는 11x11, GoogLeNet의 경우는 7x7 이었다. 반면, VGG는 처음부터 끝까지 3x3의 필터 크기를 사용하여 좋은 성과를 거두었다. 3x3 convolution 연산을 쌓는 것이 어떤 의미가 있는 것일까?\n",
    "\n",
    "3x3 convolution을 두 번 쌓는 것은 5x5 convolution과 동일한 receptive field의 정보를 처리하고, 세 번 쌓는 것은 7x7의 receptive field의 정보를 처리한다. 하지만, 3x3을 여러번 쌓는 것이 더 좋은 성능을 낸다. 그 이유는 두 가지이다. 첫째로 3x3 convolution 연산을 여러번 하는 것은 여러번의 비선형 처리를 해주는 것이므로, 큰 필터로 한번 연산 했을 때보다 더 많은 비선형성을 가질 수 있다. 두번째로는 그럼에도 불구하고, 파라미터 수는 3x3 convolution을 여러번 했을 때 더 적다. 예를 들어, 채널의 갯수가  𝐶 라고 할 때, 7x7의 경우는  72×𝐶2 의 파라미터를 가지지만, 3x3을 세 번 쌓은 경우에는,  3×(32×𝐶2) 의 파라미터를 가진다.\n",
    "\n",
    "다음에 나오는 VGG16의 summary 결과를 보면, 어마하게 많은 파라미터가 존재한다는 것을 알 수 있다. VGGNet은 간단한 구조를 가졌지만, fully connected layer가 3개가 있고, 풀링(pooling)을 거친 뒤에는 피쳐맵의 갯수가 2배로 커지면서 필요한 파라미터가 과도하게 많아졌다. 파라미터가 많다는 것은 딥러닝의 고질적인 문제인, gradient vanishing, 과적합등의 문제가 발생할 가능성이 크다는 의미이다.\n",
    "\n",
    "실제로 VGG16과 VGG19는 학습에 어려움이 있었다. 논문 저자들은 이 문제를 해결하기 위해, 표 18.4.2 의 \"A\" 모델로 학습한 fully connected layer의 가중치를 초기값으로 주어 16, 19개의 layer의 모델들을 학습시켰다.\n",
    "\n",
    "출처 : [데이터 사이언스 스쿨](https://datascienceschool.net/view-notebook/47c57f9446224af08f13e1f3b00a774e/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50\n",
    "\n",
    "![](https://i.stack.imgur.com/XTo6Q.png)\n",
    "\n",
    "ResNet(2015)은 2015년 ILSVRC에서 오류율 3.6%로 1등을 차지했습니다. 인간의 분류 오차가 5~10% 정도라는 걸 감안하면 놀라운 성적표입니다.\n",
    "\n",
    "사실 AlexNet이 처음 제안된 이후로 CNN 아키텍처의 층은 점점 더 깊어졌습니다. AlexNet이 불과 5개 층에 불과한 반면 VGGNet은 19개 층, GoogleNet은 22개 층에 달합니다. 하지만 층이 깊어질 수록 역전파되는 그래디언트가 중간에 죽어서 학습이 잘 되지 않는 문제(gradient vanishing)가 발생했습니다. ResNet 저자들이 제시한 아래 학습그래프를 보면 이같은 문제가 뚜렷이 나타납니다.\n",
    "\n",
    "![](https://i.imgur.com/zJm2b5g.png)\n",
    "\n",
    "ResNet 저자들의 핵심 아이디어는 다음 그림과 같은 residual block입니다. 그래디언트가 잘 흐를 수 있도록 일종의 지름길(shortcut, skip connection)을 만들어 주자는 생각입니다. 이는 forget gate 등을 도입해 이전 스텝의 그래디언트(정보)를 좀 더 잘 흐르게 만드려는 Long Term Short Memory(LSTM)의 철학과 본질적으로 유사합니다.\n",
    "\n",
    "![](https://i.imgur.com/fse3Ntq.png)\n",
    "\n",
    "ResNet의 성능이 좋은 이유는 그래디언트 문제 말고 또 있습니다. Veit et al. (2016)은 residual block이 앙상블(ensemble) 모델을 구축한 것과 비슷한 효과를 낸다고 주장했습니다. residual block의 skip connection 덕분에 입력데이터와 그래디언트가 오갈 수 있는 통로가 크게 늘어나기 때문입니다. (n개 skip connection이 있다면 2n개의 서로 다른 통로 존재) 이를 직관적으로 나타낸 그림은 아래 그림과 같습니다.\n",
    "\n",
    "![](https://i.imgur.com/CjLtXb0.png)\n",
    "\n",
    "출처 : [ratsgo's blog](https://ratsgo.github.io/deep%20learning/2017/10/09/CNNs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## MobileNet\n",
    "\n",
    "### 딥러닝 경량화 기술의 동향\n",
    "\n",
    " 최근 좋은 성과를 내는 딥러닝을 모바일 디바이스, 산업용 게이트웨이, IoT 센서와 같은 온-디바이스에서 작동시키기 위하여 경량화 연구가 활발히 진행중이다. 경량 딥러닝 기술이란, 알고리즘 자체를 적은 연산량을 가지게게 효율적인 구조로 설계하는 기술이다. 메모리 크기가 작은 디바이스에서 기존의 학습된 모델의 정확도를 유지시키는 것을 골자로 가장 일반화된 합성곱 신경망(CNN: Convolutional Neural Network)을 통해 다양한 연구가 진행중이다.\n",
    "\n",
    "첫번째로는 다양한 신규 계층 구조를 설계하여 신경망 구조를 제공함으로써 우수한 추론 성능을 보이는 연구가 소개되고 있다. 이는 기본 단일 층별 연산에 그치지 않고 연산량 과 파라미터 수를 줄이기 위한 잔여 블록(Residual Block) 또는 병목 블록(Bottleneck Block)과 같은 형태를 반복적으로 쌓아 신경망을 구성하는 방법이다. \n",
    "\n",
    "두번째로는 CNN 계열의 모델에서 학습 시 가장 큰 연산량을 요구하는 합성곱 연산을 줄이기 위한 효율적인 합성곱 필터 기술을 연구하는 것이다.\n",
    "\n",
    "마지막으로는 기존 신경망의 모델 구조를 인간에게 의존적으로 수행하지 않고 모델 구조를 자동 탐색함 으로써 모델을 자동화하거나 연산량 대비 모델 압축비율을 조정하는 등 다양한 자동탐색 기술이 존재한다. 이는 모바일 딥러닝과 같은 다양한 기기의 성능대비 추론속도가 중요한 응용을 위해 정확도, 지연시간, 에너지 소모량들을 사용하여 강화학습(Reinforcement Learning)을 활용하여 경량 모델을 탐색하는 기술이다.\n",
    "\n",
    "\n",
    "<img src='./img/theory/dl-trend.png' width=70%>\n",
    "\n",
    "[출처:https://ettrends.etri.re.kr/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet에 대해서\n",
    "\n",
    "- MobileNet V1 : https://arxiv.org/pdf/1704.04861.pdf\n",
    "    - 관련 Slideshare : https://www.slideshare.net/JinwonLee9/mobilenet-pr044\n",
    "- MobileNet V2 : https://arxiv.org/pdf/1801.04381.pdf\n",
    "    - 관련 Slideshare : https://www.slideshare.net/JinwonLee9/pr108-mobilenetv2-inverted-residuals-and-linear-bottlenecks\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F9917F7395C32E8EF10)\n",
    "\n",
    "Mobilenet에서는 컴퓨터 성능이 제한된 모바일, 디바이스 등에서 사용될 목적으로 설계된 CNN 구조입니다. Xception에서 배웠던 **`Depthwise separable convolution`** 이 사용하여 같은 레이어 수의 다른 CNN 구조에 비해 파라미터 수를 상당히 낮추었습니다. 기존의 합성곱 필터를 채널(Channel) 단위로 먼저 합성곱(depthwise convolution)을 하고, 그 결과를 하나의 픽셀(point) 단위로 진행 하는 합성곱(pointwise convolution)으로 분리(factorized)하여 연산량을 줄이는 방법을 제안하였습니다. 그로 인해 정확도는 어느정도 유지하면서 연산량을 8~9배로 줄이는 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "MobileNet이 작동하는 전체적인 파이프라인은 아래와 같습니다.\n",
    "\n",
    "<img src='./img/theory/mobilnetpipeline.png' width=70%>\n",
    "\n",
    "- Depthwise convolution: 깊이 별로 $3\\times3$ kernel과 합성곱을 수행합니다.\n",
    "- Pointwise convolution: 깊이 별로 합성한 결과를 $1x1$ convolution 을 사용하여 결합을 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Convolution VS Depthwise Separable Convolution\n",
    "\n",
    "#### Standard Convolution \n",
    "\n",
    "그럼 모바일넷이 기존의 합성곱 연산과 어떻게 다른지 알아보기 위해서, 다시 기존의 합성곱 연산에 대해서 살펴보자\n",
    "\n",
    " <img src='./img/theory/standard_conv.png' width=70%>\n",
    " \n",
    " - ${D_F}{\\times}{D_F}\\times{M}$(폭x높이x채널)의 크기를 같는 입력 값(Feature map)에 ${D_k}\\times{D_k}\\times{M}\\times{N}$(폭x높이x채널x갯수)의 크기를 같는 필터(kernel)을 합성곱(convolution) 시킨다면 ${D_G}\\times{D_G}\\times{N}$(폭x높이x채널)의 크기를 같는 결과값(output)이 나옵니다. <P>\n",
    " \n",
    " - 계산 비용은 ${D_k}\\times{D_k}\\times{M}\\times{N}\\times{D_F}\\times{D_F}$을 가지게 됩니다. \n",
    "\n",
    "#### Depthwise Separable Convolution\n",
    "MobileNet의 Depthwise separable convolution연산은 Xception과 대부분이 동일하고, depthwise convolution 연산과 pointwise convolution 연산 사이에도 batch normalization과 ReLU 활성화 함수 단계가 추가된 점만 다르다.\n",
    " Depthwise separable convolution은 채널(Channel) 단위로먼저 합성곱(depthwise convolution)을 하는 단계와 하나의 픽셀(point) 단위로 진행 하는 합성곱(pointwise convolution)을 하는 두 단계로 나눌 수 있습니다.\n",
    " \n",
    "> `Depthwise Separable Convolution` = `Depthwise Convolution` + `Pointwise Convolution`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Depthwise Convolution  \n",
    "\n",
    " <img src='./img/theory/STEP1.png' width=80%>\n",
    " \n",
    " - ${D_F}{\\times}{D_F}\\times{M}$(폭x높이x채널)의 크기를 같는 입력 값(Feature map)에 ${D_k}\\times{D_k}\\times{M}\\times{1}$(폭x높이x채널x갯수)의 크기를 같는 필터_1(kernel)을 합성곱(convolution) 시킨다면 ${D_F}\\times{D_F}\\times{M}$(폭x높이x채널)의 크기를 같는 결과값(output_1)이 나옵니다.<p>\n",
    " \n",
    " - 깊이(depth)별로 하므로 최종 결과값의 크기가 같은 것을 확인 할 수 있습니다. <p>\n",
    " \n",
    " - STEP1의 계산비용은 ${D_k}\\times{D_k}\\times{M}\\times{D_F}\\times{D_F}$ 이 됩니다. <p>\n",
    "    \n",
    "    \n",
    "##### Pointwise Convolution\n",
    "\n",
    "<img src='./img/theory/STEP2.png' width=80%>\n",
    "\n",
    " - ${D_F}{\\times}{D_F}\\times{M}$(폭x높이x채널)의 크기를 같는 입력 값(Output_1)에 ${1}\\times{1}\\times{M}\\times{N}$(폭x높이x채널x갯수)의 크기를 같는 필터_2(kernel)을 합성곱(convolution) 시킨다면 ${D_G}\\times{D_G}\\times{N}$(폭x높이x채널)의 크기를 같는 결과값(Output_2)이 나옵니다.<p>\n",
    " \n",
    " - ${D_F}X{D_F}X{M}$ X $1X1XN$( 1X1 conv. M번 사용하여 필터갯수 N 번 만큼 하였다.  <p>\n",
    " \n",
    " - STEP2의 계산비용은 ${M}\\times{N}\\times{D_F}\\times{D_F}$ 이 됩니다. <p>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 효율성\n",
    " 일반적인 합성곱을 두 단계로 나누어 계산하는 것은 계산량이 더 높아지는 것이라고 생각되기 쉽습니다. 하지만, 일반적인 합성곱 과 Depthwise separable convolution을 수식적으로 비교하여 본다면 필터(kernel)의 크기가 3으로 고정될때, 8-9배가 빨라짐을 확인 할 수 있습니다. \n",
    "\n",
    "<img src='./img/theory/efficiency.png' width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNetV2\n",
    "\n",
    "Linear bottleneck, Inverted residual block 을 사용하여 Mobilenet v1을 개선 시킨 Mobilenet v2 모델을 제안하였다. \n",
    " \n",
    "\n",
    "### Pipeline\n",
    " <img src='./img/theory/mobilenetv2.png'>\n",
    "    저 차원(low-dimention) 입력값을 받아 $1\\times1$ convolution 을 사용하여 채널을 늘린다. 그 후 Mobilenet v1 과 같은 깊이별 합성곱(depthwise convolution), 픽셀별 합성곱(pointwise convolution) 프로세스를 거치게 된다. (단, t는 채널을 늘리는 expansion factor )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Bottlenecks \n",
    "\n",
    "  뉴럴네트워크에서 입력 값의 집합(= input set, manifold of interest)이 저차원(low-dimension)으로 임베디드(embedded) 될 수 있다는 가정은 오래된 가정이다. Mobilenet v2에서도 이와 같은 두 가지 가정이 만족된다면, Linear Bottlenecks layer 을 가지고 Manifold of interest 부분을 포착 할 수 있다고 주장한다.\n",
    " \n",
    "#### Two assumptions (non-linear function 을 똑같이 사용 할 수 있다 )\n",
    " - Manifold of interest 부분이 ReLU를 거친 후에도 non-zero volume을 가지고 있다면 그것은 linear transformation에 해당된다.\n",
    " - Input manifold가 input의 low-dimensional subspace에 놓여 있다면, ReLU가 input manifold에 대해서 완전한 정보를 preserving할 수 있다.\n",
    "\n",
    "#### In experiments\n",
    " \n",
    " 본 논문에서는 linear layer를 사용하여 non-linearity의 정보손실(information loss)을 보존하였다. 실제 실험을 통해서 bottleneck 구조에서 non-linear layer가 성능저하를 불러왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Residuals \n",
    "\n",
    "#### Residual block VS Inverted residual block \n",
    " <img src='./img/theory/residual_inverted.png' width=90%>\n",
    "\n",
    " - Residual block\n",
    " > bottleneck이 후 expansion 이 이루어 진다. <p>\n",
    " > Residual block은 channel이 큰 block 끼리 연결되어있다.\n",
    "    \n",
    " - Inverted residual block\n",
    " > expansion 후 bottleneck 이 이루어 진다.<p>\n",
    " > Inverted residual block은 bottleneck끼리 연결되어 있다. <p>\n",
    "    (* 이를 Short cut-connection이라 하며 사용하는 이유는 ResNet처럼 Gradient back-propagation이 잘 되기 때문이다.) \n",
    " \n",
    "#### 확장(Expansion)이 필요한 이유는?\n",
    " 활성함수(Activation function) 으로 ReLu를 사용 할 시 정보손실(information loss) 가 발생하는 점을 간과 할 수 없기 때문이다. 그리하여 보통의 채널(Channel)을 감소(reduction)하는 방식이 아닌 먼저 채널을 늘리는(expanding) 방식을 사용한다. 그 결과, 채널(Channel)이 깊어졌기 때문에 저차원(low-dimensional subspace)으로 맵핑 한다하여도 정보손실을 크게 유발하지 않을 수 있으며 정보를 보존 할 수 있다. \n",
    "\n",
    "#### Structure of inverted residual block\n",
    "  <img src='./img/theory/structure of inverted residual block.png' width= 70%>\n",
    "  \n",
    "  - STEP1; Expansion Convolution \n",
    "  \n",
    "    높이가 h, 폭 w 이고 채널수 k 일때, 입력 값(Input) $h\\times w\\times k$은 convolution을 거쳐 $h\\times w\\times tk$이라는 결과값을 가지게 된다. 채널이 t배만큼 확장된 결과값을 가지며 $5 \\le t \\le 10 $ 이여야되며, defualt 값으로 6을 사용하였다. <p> \n",
    "    \n",
    "  - STEP2; Depthwise Convolution\n",
    "    \n",
    "    $3\\times3 $ kernel 을 가지고 깊이별(depthwise) 합성곱(convolution)을 실시한다. stride는 s 로 결과값으로 $\\frac{h}{s} \\times \\frac{w}{s} \\times tk $을 가지게 된다. <p>\n",
    "     \n",
    "  - STEP3; Pointwise Convolution \n",
    "   \n",
    "    픽셀별(pointwise) 합성곱으로 인해 STEP2의 결과값을 합쳐 주게 된다. \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Efficient Inference\n",
    "\n",
    "#### Bottleneck Residual Block\n",
    " <img src='./img/theory/Bottleneck Residual Block.png' width=100%>\n",
    " Bottleneck 함수를 $F(x) = \\Sigma{B \\circ N \\circ A}$ 로정의 할 수 있다. <p> - $A$ 는 linear transformation $A: R^{s \\times s \\times k} \\rightarrow R^{s \\times s \\times n}$ 로써 expansion conv.에 해당된다.  <p> - $N$ 는 non-linear transformation $N: R^{s \\times s \\times n} \\rightarrow R^{s' \\times s' \\times n}$ 로써 $N = $ReLu6$\\circ$ dwsie$\\circ$ RELu6 이며 per-channel transormation 이다. <p> - $B$ 는 linear transformation $B: R^{s' \\times s' \\times n} \\rightarrow R^{s' \\times s' \\times k'}$ pointwise conv. (= compression conv.) 이다.\n",
    " \n",
    "#### Compute graph G \n",
    " \n",
    " 병렬구조인 그래프로 메모리를 추론해본다면 $\\mbox{max}_{op \\in G}[\\sum_{A \\in op} \\left\\vert A \\right\\vert + \\sum_{B \\in op} \\left\\vert B \\right\\vert + \\left\\vert op \\right\\vert]$ 이다.\n",
    " \n",
    " 만약 우리가 Bottleneck Residual Block 을 하나의 오퍼레이터로 본다면 전체 메모리는 메모리는 bottleneck tensor 사이즈에 영향을 받는 것을 뜻한다.\n",
    " \n",
    " 즉, 입력값과 아웃풋의 사이즈가 작을수록 메모리를 적게 사용한다.\n",
    " \n",
    "#### t-way split \n",
    " \n",
    "  전체 inner tensor $I$는 $t$개의 tensor 들의 합으로 나타낼수 있다. 따라서 우리의 함수를 다음과 같이 표시 할 수 있다. $$F(x) =  \\Sigma{B_i \\circ N \\circ A_i}$$ \n",
    "  그리하여 $ F(x)$ 를 $\\frac{n}{t}$ 으로 쪼개어 학습시킬수 있지만, 너무 세분하게 쪼갤 경우 cash 메모리 부족으로 오히려 런타임이 증가할수 있기에 $ 2  \\le t  \\le 5$ 를 권장한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 성능\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 여러 CNN 모델 파라미터 및 성능 비교\n",
    "\n",
    "![](https://miro.medium.com/max/1840/1*ZqkLRkMU2ObOQWIHLBg8sw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Keras에서 제공하는 모델들\n",
    "\n",
    "직접 모델을 구성하여 컴파일해도 되지만, Keras에서 제공하는 어플리케이션 모듈에는 아래 모델을 미리 제공되기 때문에 이걸 활용해도 된다.\n",
    "\n",
    "### ImageNet으로 학습한 가중치를 이용해 이미지 분류를 수행하는 모델:\n",
    "- Xception\n",
    "- VGG16\n",
    "- VGG19\n",
    "- ResNet, ResNetV2, ResNeXt\n",
    "- InceptionV3\n",
    "- InceptionResNetV2\n",
    "- `MobileNet`\n",
    "- `MobileNetV2`\n",
    "- DenseNet\n",
    "- NASNet\n",
    "\n",
    "### 예시\n",
    "```python\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "model = MobileNet(weights='imagenet')\n",
    "\n",
    "img_path = 'elephant.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# 결과를 튜플의 리스트(클래스, 설명, 확률)로 디코딩합니다\n",
    "# (배치 내 각 샘플 당 하나의 리스트)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "# 예측결과: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]\n",
    "```\n",
    "\n",
    "출처 : [Keras 공식 홈페이지](https://keras.io/ko/applications/#_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Standard Convolution  \n",
    "\n",
    "#### 디지털 이미지\n",
    " <img src='./img/theory/rgb_image.png' width=100%>\n",
    " 위 그림과 같이 컬러 디지털 이미지는 픽셀들의 결합으로 만들어집니다. 이러한 픽셀들은 채널들의 조합으로 표현되어지며, RGB, YUV, Ycbcr 등 채널을 나누는 다양한 방법이있습다. \n",
    "\n",
    "#### Convolution Neural Network\n",
    "\n",
    " <img src='https://kr.mathworks.com/content/mathworks/kr/ko/solutions/deep-learning/convolutional-neural-network/jcr:content/mainParsys/band_copy_copy_14735/mainParsys/columns_1606542234_c/2/image.adapt.full.medium.jpg/1556019841908.jpg' width = 70%>\n",
    " \n",
    " Convolution Neural Network(CNN)에서는 이미지분류, 텍스트 또는 사운드를 분류하는 딥러닝에서 가장 많이 사용되는 알고리즘입니다. 특히, 패턴을 찾는데 유용하며 CNN을 학습하면 이미지를 분류하고 특징을 수동으로 추출할 필요가 없으며 높은 정확도를 보인다는 장점 이있습니다. 이러한 장점들로 인하여 최근 CNN의 사용하는 사례가 급증하게 되었습니다.<p> \n",
    "\n",
    " CNN은 이미지의 픽셀값을 입력값으로 받아 합성곱$ \\rightarrow$ ReLu $\\rightarrow$ Pooling을 반복 적으로 수행하여 특징을 추출하게 됩니다. \n",
    "\n",
    "<img src='https://kr.mathworks.com/content/mathworks/kr/ko/solutions/deep-learning/convolutional-neural-network/jcr:content/mainParsys/band_copy_copy_14735_1026954091/mainParsys/columns_1606542234_c/2/image.adapt.full.medium.jpg/1556019842434.jpg' with= 70%>\n",
    "\n",
    " - 합성곱: 가장 중요한 레이어인 합성곱(Convolution) 층(layer)에서는 3차원의 공간정보를 학습하게 됩니다. <p>\n",
    " \n",
    " - ReLu(Rectified Linear Unit): 다양한 활성함수(Actication Function)이 있지만 주로 ReLu를 사용하며 비선형적인 특징때문에 층을 깊게 쌓을수 있도록 해줍니다. <p>\n",
    " \n",
    " - Pooling: 네트워크에서 학습해야하는 매개변수 수를 줄여서 출력을 간소화 시켜주는 역할을 합니다.<p>\n",
    "\n",
    "\n",
    "### Process of Convolution \n",
    "\n",
    "<img src='https://camo.githubusercontent.com/bd17725182187a746c1d7a49ed1c3e3722f2b832/68747470733a2f2f7777772e636e746b2e61692f6a75702f636e746b313033645f636f6e7632645f66696e616c2e676966' width=30%>\n",
    "\n",
    "### ReLu6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T07:05:27.543319Z",
     "start_time": "2019-11-09T07:05:27.429479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD4CAYAAADIH9xYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAXc0lEQVR4nO3de4xc91nG8e+7N9/Wl9i7O3YTJ04cx5cNzWXdpmlTJ5uktqFAoSqoSCAKSAYJUJFAogUhQAghJChUlCIKVK2gsKBCaZW2M07SddPQS+pNk3bGlzixncSOz67t9doer/f+8seO0613vTM7OzPnMs9HGnkv56zeV2fn2fFvzjmvuTsiIhJ9DWEXICIipVFgi4jEhAJbRCQmFNgiIjGhwBYRiYmmavzQtrY237RpU1n7XrlyhRUrVlS2oBAlrR9IXk9J6weS11PS+oHZPfX19Z1z9/Z5d3L3ij+6urq8XL29vWXvG0VJ68c9eT0lrR/35PWUtH7cZ/cEHPQi2aolERGRmFBgi4jEhAJbRCQmFNgiIjGhwBYRiYmSAtvM1pjZ583siJkdNrMHq12YiIj8qFLPw/44kHb3D5hZC7C8ijWJiMgciga2ma0GdgEfAnD3MWCsumWJRM9/fvc1Tl+4GnYZJTn56hjPjx0Nu4yKiUs/y5c08RsPb67azzcvcj9sM7sX+BRwCLgH6AM+7O5XrttuH7APIJVKdfX09JRVUD6fp7W1tax9oyhp/UDyeiqlnzP5KT767HRYWy2KWjQnLpWWJh79rFpifLy7tAWI63/vuru7+9x957w7FbuyBtgJTAAPFD7/OPBn8+2jKx1/KGn9uCevp1L6+cTXjvltv/+EvzE0XP2CKqAej1HcVOtKx1PAKXf/TuHzzwP3l/QnRCQh9ucC7tm4hg2rl4VditSxooHt7gHwupltLXzpMaaXR0TqwhtDV3nx1EX2dq4PuxSpc6WeJfLbwOcKZ4gcB36leiWJRMv+XADAns5UyJVIvSspsN39BabXskXqTjoXcFeqlTvak/NGq8STrnQUmcf5/CjPnRhkj5ZDJAIU2CLzePrwAFOOAlsiQYEtMo90LuDmNcvofMuqsEsRUWCL3Eh+dIJnj51j793rMYv+RRuSfApskRvoPTLA2OQUe+/WcohEgwJb5AYyuYC21hbuv/WmsEsRARTYInMaGZ+k98gA79mxnsYGLYdINCiwRebwzVfOcWVsUhfLSKQosEXmkM4GrFzSxDs3t4VdisibFNgi15mYnOLJQ/08ur2DliY9RSQ69Nsocp3vnrzAheFx3exJIkeBLXKdTC5gSVMDD29tD7sUkR+hwBaZwd3J5AJ23dXO8pZSb2YpUhsKbJEZvn/qImcujujeIRJJCmyRGTK5gMYG4/HtHWGXIjKLAlukwN1JZwPeccda1ixvCbsckVkU2CIFLw/kOX7uis4OkchSYIsUZAqjwHYrsCWiFNgiBelcwH23riG1amnYpYjMSYEtApy6MEz29CUth0ikKbBFgEyuH9AoMIk2BbYI0+vX29avZFPbirBLEbkhBbbUvUujzndPDurNRok8BbbUve8NTOCO1q8l8kq6WYKZnQQuA5PAhLvvrGZRIrXU1z/JxrXL2L5hZdiliMxrIXe36Xb3c1WrRCQEl0bGyZ2f5Fcf0mR0iT4tiUhd6z0ywKTr7BCJB3P34huZnQAuAA78o7t/ao5t9gH7AFKpVFdPT09ZBeXzeVpbW8vaN4qS1g8kq6dPfG+Eo4MTfPzRFTQk6BV2ko4RJK8fmN1Td3d3X9HlZncv+gBuLvzbAbwI7Jpv+66uLi9Xb29v2ftGUdL6cU9OT1fHJnz7H33VP/SJdNilVFxSjtE1SevHfXZPwEEvksUlLYm4++nCvwPAF4C3l/EHRSRSvnHsHMNjk3SlGsMuRaQkRQPbzFaY2cprHwO7gWy1CxOptkwuYNXSJratVWBLPJRylkgK+ELhHfQm4N/dPV3VqkSqbGJyiqcO9/PY9hRNDUNhlyNSkqKB7e7HgXtqUItIzTx3YpCh4fHps0POKbAlHnRan9SldC5gaXMDD9+lyegSHwpsqTtTU9OT0R++q51lLVq/lvhQYEvdefHUEP2XRnWxjMSOAlvqTjoX0NRgPLYtFXYpIguiwJa64u5ksgEPbl7H6uXNYZcjsiAKbKkrL/XnOXl+WMshEksKbKkrmVyAGezeoeUQiR8FttSVdDbg/ltvokOT0SWGFNhSN14fHObQGU1Gl/hSYEvdyOQCQPe+lvhSYEvdSGcDtm9Yxa3rloddikhZFNhSFwYuj9D32gX2dOrNRokvBbbUhScP9U9PRr9byyESXwpsqQuZXD+b1i1na0qT0SW+FNiSeBevjvPNl8+xp1OT0SXeFNiSeL1HBpiYcvZoOURiToEtiZfOBnSsXMK9t6wJuxSRRVFgS6JdHZvkwEsD7OlcT0ODlkMk3hTYkmjPHDvLyPiULpaRRFBgS6JlsgGrlzXzwB1rwy5FZNEU2JJY44XJ6I9vT9HcqF91iT/9Fktiffv4eS6NTOjqRkkMBbYkViYXsKy5kV2ajC4JocCWRJqacvbn+nlkaztLmzUZXZKh5MA2s0Yz+56ZPVHNgkQq4XuvDzFweVT3DpFEWcgr7A8Dh6tViEglZXIBzY1G97aOsEsRqZiSAtvMbgHeC/xzdcsRWTx3J50NeOfmNlYt1WR0SQ5z9+IbmX0e+AtgJfB77v6Tc2yzD9gHkEqlunp6esoqKJ/P09raWta+UZS0fiD6Pb1+eYo/+r+rfKizhUc2Fg/sqPdTjqT1lLR+YHZP3d3dfe6+c96d3H3eB/CTwCcLHz8CPFFsn66uLi9Xb29v2ftGUdL6cY9+Tx/bf9Q3feQJH7g0UtL2Ue+nHEnrKWn9uM/uCTjoRbK1lCWRdwE/bWYngR7gUTP7tzL+oIjURCYX8Lbb1tK+cknYpYhUVNHAdvePuvst7r4J+CDwNXf/xapXJlKGV89f4Uhwmd26WEYSSOdhS6JoMrokWdNCNnb3A8CBqlQiUgHpbEDnW1axca0mo0vy6BW2JEb/pRGef22IvXp1LQmlwJbE2H+oH0CjwCSxFNiSGJlswB1tK9jSkazzdUWuUWBLIgwNj/Ht4+fZrcnokmAKbEmEpw9PT0bXzZ4kyRTYkgiZXMCG1Ut5682rwy5FpGoU2BJ7w2MTfP2ls+zekdJkdEk0BbbE3jMvnWV0Ykpnh0jiKbAl9tLZgJuWN/P2TZqMLsmmwJZYG5uY4ukjAzy+PUWTJqNLwuk3XGLtW8fPc3lkQvcOkbqgwJZYS2cDlrc08tCWtrBLEak6BbbE1uSU8+Shfrq3dmgyutQFBbbE1vOvXeBcflRnh0jdUGBLbGWyAS2NDXRvbQ+7FJGaUGBLLLk76VzAu+5cx0pNRpc6ocCWWDp05hKnLlzVvUOkriiwJZYy2YAGg8e3a3aj1A8FtsRSOhfwtk1rWdeqyehSPxTYEjvHz+Z5qT+vi2Wk7iiwJXYyOY0Ck/qkwJbYyeQC3nrLam5esyzsUkRqSoEtsRJcHOGF14e0HCJ1SYEtsbL/UACgwJa6VDSwzWypmT1nZi+aWc7M/rQWhYnMJZ0N2Ny+gjs1GV3qUCmvsEeBR939HuBeYK+ZvaO6ZYnMduHKGN85MaiLZaRuNRXbwN0dyBc+bS48vJpFiczlqcP9TE65lkOkbtl0HhfZyKwR6APuBP7e3X9/jm32AfsAUqlUV09PT1kF5fN5WluT89/dpPUD4fX0t30jvHZ5ir9+eBlmlRu2q2MUfUnrB2b31N3d3efuO+fdyd1LfgBrgF7g7vm26+rq8nL19vaWvW8UJa0f93B6yo+M+5Y//Ir/8RezFf/ZOkbRl7R+3Gf3BBz0Ihm8oLNE3H2oENh7F/jHRGRRDhw9y9jElNavpa6VcpZIu5mtKXy8DHgPcKTahYnMlMkFrF3Rwts0GV3qWNE3HYENwGcL69gNwH+5+xPVLUvkh0YnJvnakQHe+2MbaGyo3Nq1SNyUcpbI94H7alCLyJy++cp58qMTWg6RuqcrHSXyMtmA1iVNvPPOdWGXIhIqBbZE2puT0bd1sKRJk9GlvimwJdIOnhzk/JUx9nRqsoyIAlsiLZPrp6WpgUe2doRdikjoFNgSWe5OJhfw7jvbaF1SyglNIsmmwJbIyr1xidNDVzVZRqRAgS2RldZkdJEfocCWyMrkAh64fR1rV7SEXYpIJCiwJZJeHshzbCCvs0NEZlBgSyRlctOjwHbr3tcib1JgSyTtzwXcc8tq3qLJ6CJvUmBL5LwxdJUXT13U2SEi11FgS+Tsz2kyushcFNgSOelcwJaOVja3J2sklMhiKbAlUgavjPHciUG9uhaZgwJbIuWpQ/1MObr3tcgcFNgSKZlcwM1rltH5llVhlyISOQpsiYz86ATfOHaOPZ3rMdMoMJHrKbAlMg4cHWBsUpPRRW5EgS2Rkc4GrFvRQtdtN4VdikgkKbAlEkbGJ+k9MsDuzpQmo4vcgAJbIuGbr5zjytik7h0iMg8FtkRCJtvPyiVNvHOzJqOL3IgCW0I3MTnFk4c1GV2kmKKBbWYbzazXzA6ZWc7MPlyLwqR+HHz1AoNXxnR2iEgRpUw2nQB+192fN7OVQJ+ZPenuh6pcm9SJdDagpamBh+9qD7sUkUgr+grb3c+4+/OFjy8Dh4Gbq12Y1Ad3Z38uYNeWdlZoMrrIvMzdS9/YbBPwDHC3u1+67nv7gH0AqVSqq6enp6yC8vk8ra3JuUtb0vqByvZ04uIkf/qtEX7t7hbefUtzRX7mQukYRV/S+oHZPXV3d/e5+855d3L3kh5AK9AHvL/Ytl1dXV6u3t7esveNoqT1417Znv7yq4f9jo9+2QfzoxX7mQulYxR9SevHfXZPwEEvkq0lnSViZs3AfwOfc/f/KfMPisgs05PR13KTJqOLFFXKWSIG/Atw2N0/Vv2SpF68PHCZV85e0dkhIiUq5RX2u4BfAh41sxcKj5+ocl1SBzK5fgB271Bgi5Si6Nvy7v4soJs7SMWlswH3blzD+tVLwy5FJBZ0paOE4vTQVX5w+qKWQ0QWQIEtochkNRldZKEU2BKKdC5ga2olt7etCLsUkdhQYEvNncuPcvDkIHs6U2GXIhIrCmypuWuT0fdo/VpkQRTYUnOZXMDGtcvYsUGT0UUWQoEtNXV5ZJz/e/k8e3ZoMrrIQimwpaZ6j57VZHSRMimwpaYy2YC21iXcf6smo4sslAJbamZkfJLeo9OT0Rs0GV1kwRTYUjPPHjvH8NikLpYRKZMCW2omnQtYubSJB+/QZHSRciiwpSYmJqd46nA/j23roKVJv3Yi5dAzR2riuRODDA2P6+wQkUVQYEtNZHIBS5sb2KXJ6CJlU2BL1U1NOZlcP7u2tLO8RZPRRcqlwJaq+/7piwSXRrQcIrJICmypunQ2oKnBeGyb7s4nshgKbKkqdyeTC3hw8zpWL28OuxyRWFNgS1UdG8hz4twVdutiGZFFU2BLVaWzAWawZ4eWQ0QWS4EtVZXJBdy3cQ0dqzQZXWSxFNhSNa8PDpN745LODhGpEAW2VE0mp8noIpVUNLDN7NNmNmBm2VoUJMmRyQVsW7+S29ZpMrpIJZTyCvszwN4q1yEJc/byKAdfvaDlEJEKKhrY7v4MMFiDWiRBnjzUj7uWQ0Qqydy9+EZmm4An3P3uebbZB+wDSKVSXT09PWUVlM/naW1tLWvfKEpaP1BaT391cISB4Sn+8t3LIj9st16PUZwkrR+Y3VN3d3efu++cdyd3L/oANgHZUrZ1d7q6urxcvb29Ze8bRUnrx714T0PDY37nH3zZ//zLh2pT0CLV4zGKm6T14z67J+CgF8lWnSUiFdd7ZIDxSddyiEiFKbCl4jK5gI6VS7hv45qwSxFJlFJO6/sP4FvAVjM7ZWa/Vv2yJK5Gxic5cPSsJqOLVEHRu8m7+y/UohBJhmdeOsvV8Un2dm4IuxSRxNGSiFRUOhewelkzD9yxNuxSRBJHgS0VMz45xdOHB3hsewfNjfrVEqk0PaukYr5zfJCLV8d1dohIlSiwpWIyuYBlzY3s2qLJ6CLVoMCWipiejB7w8F3tLGtpDLsckURSYEtFvHBqiIHLo7rZk0gVKbClIjLZgOZGo3tbR9iliCSWAlsWzd1J5wIe3NzG6mWajC5SLQpsWbSj/Zd59fwwe3V2iEhVKbBl0a5NRn+PJqOLVJUCWxYtk+tn52030b5ySdiliCSaAlsW5bXzwxw+c0kXy4jUgAJbFkWT0UVqR4Eti5LOBezYsIqNa5eHXYpI4imwpWwDl0Z4/jVNRhepFQW2lG1/YTK6AlukNhTYUrZMLuD2thVs6UjWNGuRqFJgS1kuDo/zrVfOs6dzPWYaBSZSCwpsKcvXjvYzMeXs6dTFMiK1osCWsqSzAetXLeWeWzQZXaRWFNiyYKOTztdf0mR0kVpTYMuCZc9NMjI+pZs9idSYAlsW7GD/BGuWN/P22zUZXaSWFNiyIGMTU7w4MMnj21M0aTK6SE2V9Iwzs71mdtTMXjazj1S7KImubx8/z/AEWg4RCUHRwDazRuDvgR8HdgC/YGY7ql2YRFMmF7CkER7a0hZ2KSJ1p6mEbd4OvOzuxwHMrAd4H3Co0sX81N89y/mLw6x4/uuV/tGhuTKcrH5eGxzmre2NLG3WZHSRWislsG8GXp/x+Sngges3MrN9wD6AVCrFgQMHFlxM69QITUumaLSrC943qloT1s89bcYj6yfKOr5Rlc/nE9UPJK+npPUDZfbk7vM+gA8A/zzj818CPjHfPl1dXV6u3t7esveNoqT14568npLWj3vyekpaP+6zewIOepE8LuVNx9PAxhmf31L4moiI1FApgf1dYIuZ3W5mLcAHgS9VtywREble0TVsd58ws98CMkAj8Gl3z1W9MhER+RGlvOmIu38F+EqVaxERkXnoUjURkZhQYIuIxIQCW0QkJhTYIiIxYdPna1f4h5qdBV4tc/c24FwFywlb0vqB5PWUtH4geT0lrR+Y3dNt7t4+3w5VCezFMLOD7r4z7DoqJWn9QPJ6Slo/kLyektYPlNeTlkRERGJCgS0iEhNRDOxPhV1AhSWtH0heT0nrB5LXU9L6gTJ6itwatoiIzC2Kr7BFRGQOCmwRkZiIRGCb2c+ZWc7Mpsxs53Xf+2hh+O9RM9sTVo2LYWZ/YmanzeyFwuMnwq6pHEkcxmxmJ83sB4XjcjDsesphZp82swEzy8742loze9LMjhX+vSnMGhfiBv3E9jlkZhvNrNfMDhVy7sOFry/4GEUisIEs8H7gmZlfLAz7/SDQCewFPlkYChxHf+Pu9xYesbvzYcKHMXcXjktcz/P9DNPPj5k+Ajzt7luApwufx8VnmN0PxPc5NAH8rrvvAN4B/GbhubPgYxSJwHb3w+5+dI5vvQ/ocfdRdz8BvMz0UGCpvTeHMbv7GHBtGLOEzN2fAQav+/L7gM8WPv4s8DM1LWoRbtBPbLn7GXd/vvDxZeAw07NyF3yMIhHY85hrAPDNIdWyWL9lZt8v/HcvNv89nSFJx2ImB/abWV9hkHRSpNz9TOHjAEiFWUyFxP05hJltAu4DvkMZx6hmgW1mT5lZdo5HIl6lFenvH4DNwL3AGeCvQy1WZnrI3e9neqnnN81sV9gFVVphwGvcz9+N/XPIzFqB/wZ+x90vzfxeqceopIkzleDuj5exW2wGAJfan5n9E/BElcuphtgci4Vw99OFfwfM7AtML/08M/9esdBvZhvc/YyZbQAGwi5oMdy9/9rHcXwOmVkz02H9OXf/n8KXF3yMor4k8iXgg2a2xMxuB7YAz4Vc04IVDsY1P8v0m6xxk7hhzGa2wsxWXvsY2E08j81cvgT8cuHjXwa+GGItixbn55CZGfAvwGF3/9iMby34GEXiSkcz+1ng74B2YAh4wd33FL73h8CvMv1O6++4+1dDK7RMZvavTP9XzoGTwK/PWLuKjcKpVH/LD4cx/3nIJS2Kmd0BfKHwaRPw73Hsycz+A3iE6dt19gN/DPwv8F/ArUzf6vjn3T0Wb+TdoJ9HiOlzyMweAr4B/ACYKnz5D5hex17QMYpEYIuISHFRXxIREZECBbaISEwosEVEYkKBLSISEwpsEZGYUGCLiMSEAltEJCb+H6+wyRo8ZseAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu6(x):\n",
    "    return np.minimum(np.maximum(x,0),6)\n",
    "\n",
    "x = np.arange(-10,20)\n",
    "y = relu6(np.arange(-10,20))\n",
    "plt.plot(x,y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Intel\n",
    "    - https://www.intel.co.kr/\n",
    "- Intel OpenVINO\n",
    "    - https://software.intel.com/en-us/openvino-toolkit\n",
    "- MNIST\n",
    "    - http://yann.lecun.com/exdb/mnist/\n",
    "- CIFAR10\n",
    "    - https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "- ImageNet\n",
    "    - http://www.image-net.org\n",
    "- Tensorflow\n",
    "    - https://www.tensorflow.org/?hl=ko\n",
    "- Keras\n",
    "    - https://keras.io/\n",
    "    - https://tensorflow.blog/2019/03/06/tensorflow-2-0-keras-api-overview/\n",
    "    - https://tykimos.github.io/2017/02/22/Integrating_Keras_and_TensorFlow/\n",
    "    - https://tykimos.github.io/2017/03/08/CNN_Getting_Started/\n",
    "    - https://raw.githubusercontent.com/keras-team/keras-docs-ko/master/sources/why-use-keras.md\n",
    "- Keras to Caffe\n",
    "     - https://github.com/uhfband/keras2caffe\n",
    "     - http://www.deepvisionconsulting.com/from-keras-to-caffe/\n",
    "- Fully Connected Layer\n",
    "    - https://sonofgodcom.wordpress.com/2018/12/31/cnn%EC%9D%84-%EC%9D%B4%ED%95%B4%ED%95%B4%EB%B3%B4%EC%9E%90-fully-connected-layer%EB%8A%94-%EB%AD%94%EA%B0%80/\n",
    "- Convultional Nueral Network\n",
    "    - http://aikorea.org/cs231n/convolutional-networks/\n",
    "    - http://cs231n.stanford.edu/\n",
    "- CNN Models\n",
    "    - https://ratsgo.github.io/deep%20learning/2017/10/09/CNNs/\n",
    "\n",
    "- VOC2012\n",
    "    - https://blog.godatadriven.com/rod-keras-multi-label\n",
    "    - https://gist.github.com/rragundez/ae3a17428bfec631d1b35dcdc6296a85#file-multi-label_classification_with_keras_imagedatagenerator-ipynbhttps://fairyonice.github.io/Part_5_Object_Detection_with_Yolo_using_VOC_2012_data_training.html\n",
    "    - http://research.sualab.com/introduction/2017/11/29/image-recognition-overview-1.html\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "강의목차",
   "title_sidebar": "강의목차",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
